{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76983df-053b-450b-976c-295826248978",
   "metadata": {},
   "source": [
    "# Unraveling scalar mults and countermeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1528b8-61cd-4219-993f-e3f1ac79e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import binom, entropy\n",
    "from scipy.spatial import distance\n",
    "from tqdm.auto import tqdm, trange\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from anytree import PreOrderIter, Walker\n",
    "\n",
    "from pyecsca.ec.mult import *\n",
    "from pyecsca.misc.utils import TaskExecutor, silent\n",
    "from pyecsca.sca.re.tree import Map, Tree\n",
    "\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3814d6d-af42-4b9a-bbf2-6dbbdddd92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_interval(p: float, samples: int, alpha: float = 0.05) -> tuple[float, float]:\n",
    "    return proportion_confint(round(p*samples), samples, alpha, method=\"wilson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eac736-be48-4925-accf-1ca8ff6aa065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powers_of(k, max_power=20):\n",
    "    return [k**i for i in range(1, max_power)]\n",
    "\n",
    "def prod_combine(one, other):\n",
    "    return [a * b for a, b in itertools.product(one, other)]\n",
    "\n",
    "small_primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199]\n",
    "medium_primes = [211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397]\n",
    "large_primes = [401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997]\n",
    "all_integers = list(range(1, 400))\n",
    "all_even = list(range(2, 400, 2))\n",
    "all_odd = list(range(1, 400, 2))\n",
    "all_primes = small_primes + medium_primes + large_primes\n",
    "\n",
    "divisor_map = {\n",
    "    \"small_primes\": small_primes,\n",
    "    \"medium_primes\": medium_primes,\n",
    "    \"large_primes\": large_primes,\n",
    "    \"all_primes\": all_primes,\n",
    "    \"all_integers\": all_integers,\n",
    "    \"all_even\": all_even,\n",
    "    \"all_odd\": all_odd,\n",
    "    \"powers_of_2\": powers_of(2),\n",
    "    \"powers_of_2_large\": powers_of(2, 256),\n",
    "    \"powers_of_2_large_3\": [i * 3 for i in powers_of(2, 256)],\n",
    "    \"powers_of_2_large_p1\": [i + 1 for i in powers_of(2, 256)],\n",
    "    \"powers_of_2_large_m1\": [i - 1 for i in powers_of(2, 256)],\n",
    "    \"powers_of_2_large_pmautobus\": sorted(set([i + j for i in powers_of(2, 256) for j in range(-5,5) if i+j > 0])),\n",
    "    \"powers_of_3\": powers_of(3),\n",
    "}\n",
    "divisor_map[\"all\"] = list(sorted(set().union(*[v for v in divisor_map.values()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868c083-8073-453d-b508-704fcb6d6f2a",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "Select *divisor name* to restrict the features. Select *kind* to pick the probmap source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb00342-3c48-49c9-bedf-2341e5eae3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "divisor_name = \"all\"\n",
    "kind = \"all\"\n",
    "allfeats = divisor_map[divisor_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbac9be-d098-479a-8ca2-f531f6668f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "try:\n",
    "    with open(f\"{divisor_name}_{kind}_distrs.pickle\", \"rb\") as f:\n",
    "        distributions_mults = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    with open(f\"all_{kind}_distrs.pickle\", \"rb\") as f:\n",
    "        distributions_mults = pickle.load(f)\n",
    "    for probmap in distributions_mults.values():\n",
    "        probmap.narrow(allfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c81e38-a37c-4e58-ac9e-927d14dad458",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmults = len(distributions_mults.keys())\n",
    "nallfeats = len(allfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f783baf-bc81-40c1-9282-e2dfdacfd17c",
   "metadata": {},
   "source": [
    "## Build dmap and tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161f3e8-6e39-47c1-a26f-23f910f3fe26",
   "metadata": {},
   "source": [
    "Select the n for building the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307bf7c-4fac-489d-8527-7ddbf536a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbuild = 10000\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85fad7-392f-4701-9329-d75d39736bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now go over all divisors, cluster based on overlapping CI for given n?\n",
    "io_map = {mult:{} for mult in distributions_mults.keys()}\n",
    "for divisor in allfeats:\n",
    "    prev_ci_low = None\n",
    "    prev_ci_high = None\n",
    "    groups = {}\n",
    "    pvals = {}\n",
    "    group = 0\n",
    "    for mult, probmap in sorted(distributions_mults.items(), key=lambda item: -item[1][divisor]):\n",
    "        # We are going from high to low p.\n",
    "        pval = probmap[divisor]\n",
    "        pvals[mult] = pval\n",
    "        ci_low, ci_high = conf_interval(pval, nbuild, alpha)\n",
    "        ci_low = max(ci_low, 0.0)\n",
    "        ci_high = min(ci_high, 1.0)\n",
    "        if (prev_ci_low is None and prev_ci_high is None) or prev_ci_low >= ci_high:\n",
    "            g = groups.setdefault(f\"arbitrary{group}\", set())\n",
    "            g.add(mult)\n",
    "            group += 1\n",
    "        else:\n",
    "            g = groups.setdefault(f\"arbitrary{group}\", set())\n",
    "            g.add(mult)\n",
    "        prev_ci_low = ci_low\n",
    "        prev_ci_high = ci_high\n",
    "    \n",
    "    #print(f\"Divisor: {divisor}, num groups: {group}\", end=\"\\n\\t\")\n",
    "    #for g in groups.values():\n",
    "    #    print(len(g), end=\", \")\n",
    "    #print()\n",
    "    for group, mults in groups.items():\n",
    "        mult_pvals = [pvals[mult] for mult in mults]\n",
    "        group_pval_avg = np.mean(mult_pvals)\n",
    "        group_pval_var = np.var(mult_pvals)\n",
    "        group_pval_min = np.min(mult_pvals)\n",
    "        group_pval_max = np.max(mult_pvals)\n",
    "        for mult in mults:\n",
    "            io_map[mult][divisor] = (group,  group_pval_avg, group_pval_var, group_pval_min, group_pval_max)\n",
    "\n",
    "# then build dmap\n",
    "dmap = Map.from_io_maps(set(distributions_mults.keys()), io_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06104104-b612-40e9-bc1d-646356a13381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dmap.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8672ca-b76b-411d-b514-2387b555f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate dmap\n",
    "dmap.deduplicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba09b0-31c3-450b-af30-efaa64329743",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dmap.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735e7d4-149c-4184-96f7-dcfd6017fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a tree\n",
    "with silent():\n",
    "    tree = Tree.build(set(distributions_mults.keys()), dmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41093df-32c4-450d-922d-5ad042539397",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de577429-d87c-4967-be17-75cbb378860c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tree.render_basic())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bcd9c-1da5-428a-a979-0835326777f3",
   "metadata": {},
   "source": [
    "## Simulate distinguishing using a tree\n",
    "We can now simulate distinguishing using the tree and how it behaves with increasing the number of samples per divisor collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ed95f-0949-4251-874f-9b87cfe97a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = 1000\n",
    "\n",
    "for nattack in trange(100, 10000, 100):\n",
    "    successes = 0\n",
    "    pathiness = 0\n",
    "    for i in range(simulations):\n",
    "        true_mult = random.choice(list(distributions_mults.keys()))\n",
    "        probmap = distributions_mults[true_mult]\n",
    "        node = tree.root\n",
    "        while True:\n",
    "            if node.is_leaf:\n",
    "                break\n",
    "            divisor = node.dmap_input\n",
    "            prob = probmap[divisor]\n",
    "            sampled_prob = binom(nattack, prob).rvs() / nattack\n",
    "            best_child = None\n",
    "            true_child = None\n",
    "            best_group_distance = None\n",
    "            #print(f\"Divisor: {divisor}, prob: {prob}, sampled: {sampled_prob}\")\n",
    "            for child in node.children:\n",
    "                if true_mult in child.cfgs:\n",
    "                    true_child = child\n",
    "                group, group_pval_avg, group_pval_var, group_pval_min, group_pval_max = child.response\n",
    "                group_distance = min(abs(sampled_prob - group_pval_min), abs(sampled_prob - group_pval_max))\n",
    "                #print(f\"Child {group}, {group_pval_avg}\")\n",
    "                if best_child is None or \\\n",
    "                    (group_distance < best_group_distance):\n",
    "                    best_child = child\n",
    "                    best_group_distance = group_distance\n",
    "                if sampled_prob > group_pval_min and sampled_prob < group_pval_max:\n",
    "                    best_child = child\n",
    "                    break\n",
    "            #print(f\"Best {best_child.response}\")\n",
    "            if true_child is not None and true_child != best_child:\n",
    "                pass\n",
    "                #print(f\"Mistake! {prob}, {sampled_prob} true:{true_child.response}, chosen:{best_child.response}\")\n",
    "            node = best_child\n",
    "            if true_mult in node.cfgs:\n",
    "                pathiness += 1\n",
    "        #print(f\"Arrived: {true_mult in node.cfgs}\")\n",
    "        if true_mult in node.cfgs:\n",
    "            successes += 1\n",
    "    print(f\"{nattack}: success rate {successes/simulations}, pathiness {pathiness/simulations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308df683-952e-430a-b2bd-f19bcfb98b8e",
   "metadata": {},
   "source": [
    "## Simulate distinguishing using a distance metric\n",
    "\n",
    "We need to first select some features (divisors) from the set of all divisors that we will query\n",
    "the target with. This set should be the smallest (to not do a lot of queries) yet allow us to distinguish as\n",
    "much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d2f2a2-495e-459d-b0e2-89c9a5973b1e",
   "metadata": {},
   "source": [
    "### Feature selection using trees + classification error\n",
    "\n",
    "We can reuse the clustering + tree building approach above and just take the inputs that the greedy tree building choses as the features. However, we can also use more conventional feature selection approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5720a-f793-4ad9-ad27-1bd943bb325b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feats_in_tree = Counter()\n",
    "for node in PreOrderIter(tree.root):\n",
    "    if node.is_leaf:\n",
    "        continue\n",
    "    feats_in_tree[node.dmap_input] += 1\n",
    "feats_in_tree = set(feats_in_tree.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49b67c-0a52-443e-904c-98f0a3d7febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes(nattack: int, feat_vector: list[int], feats, probmap):\n",
    "    bayes.reverse = True\n",
    "    log_likelihood = 0.0\n",
    "    for sampled, divisor in zip(feat_vector, feats):\n",
    "        other_p = probmap[divisor]\n",
    "        log_prob = binom(nattack, other_p).logpmf(sampled)\n",
    "        log_likelihood += log_prob\n",
    "    return log_likelihood\n",
    "\n",
    "def euclid(nattack: int, feat_vector: list[int], feats, probmap):\n",
    "    euclid.reverse = False\n",
    "    other_vector = np.zeros(nfeats)\n",
    "    for i, divisor in enumerate(feats):\n",
    "        other_vector[i] = probmap[divisor]\n",
    "    return distance.euclidean(feat_vector, other_vector)\n",
    "\n",
    "# TODO: Adjust scorers to penalize/reject when sampled prob of a feature is != 1.0 but the mult has that feature at 1.0 proba.\n",
    "\n",
    "def one_simulation(nattack, true_mult, mults, feats, scorer,):\n",
    "    probmap = mults[true_mult]\n",
    "    feat_vector = []\n",
    "    for divisor in feats:\n",
    "        prob = probmap[divisor]\n",
    "        sampled = binom(nattack, prob).rvs()\n",
    "        feat_vector.append(sampled)\n",
    "    scoring = []\n",
    "    for other_mult, other_probmap in mults.items():\n",
    "        score = scorer(nattack, feat_vector, feats, other_probmap)\n",
    "        scoring.append((score, other_mult))\n",
    "    scoring.sort(key=lambda item: item[0], reverse=scorer.reverse)\n",
    "    for i, (sim, other) in enumerate(scoring):\n",
    "        if other == true_mult:\n",
    "            return i\n",
    "\n",
    "def many_simulations(nattack, mults, feats, scorer, simulations):\n",
    "    successes = {k:0 for k in range(1, 11)}\n",
    "    mean_pos = 0\n",
    "    mults_l = list(mults)\n",
    "    for i in range(simulations):\n",
    "        if len(mults) <= simulations:\n",
    "            true_mult = mults_l[i]\n",
    "        else:\n",
    "            true_mult = random.choice(mults_l)\n",
    "        pos = one_simulation(nattack, true_mult, mults, feats, scorer)\n",
    "        mean_pos += pos\n",
    "        for k in range(1, 11):\n",
    "            if pos + 1 <= k:\n",
    "                successes[k] += 1\n",
    "    mean_pos /= simulations\n",
    "    for i in successes.keys():\n",
    "        successes[i] /= simulations\n",
    "    return mean_pos, successes\n",
    "\n",
    "def find_features_random(feat_subset, nfeat_range, nattack_range, num_workers, feat_retries, simulations, scorer):\n",
    "    for nfeats in nfeat_range:\n",
    "        for nattack in nattack_range:\n",
    "            best_feats = None\n",
    "            best_feats_mean_pos = None\n",
    "            best_successes = None\n",
    "            with TaskExecutor(max_workers=num_workers) as pool:\n",
    "                for retry in range(feat_retries):\n",
    "                    feats = random.sample(sorted(feat_subset), nfeats)\n",
    "                    pool.submit_task(retry,\n",
    "                                     many_simulations,\n",
    "                                     nattack, distributions_mults, feats, scorer, simulations)\n",
    "                for i, future in tqdm(pool.as_completed(), leave=False, desc=\"Retries\", total=feat_retries, smoothing=0):\n",
    "                    mean_pos, successes = future.result()\n",
    "                    #print(f\"{nfeats} {nattack}({i}): mean pos {mean_pos:.2f} top1: {successes[1]:.2f}, top5: {successes[5]:.2f}, top10: {successes[10]:.2f}\")\n",
    "                    if best_feats is None or best_feats_mean_pos > mean_pos:\n",
    "                        best_feats = feats\n",
    "                        best_feats_mean_pos = mean_pos\n",
    "                        best_successes = successes\n",
    "            \n",
    "            print(f\"Best results for {nfeats} feats at {nattack} samples out of {retries} random feat subsets.\")\n",
    "            print(f\"Features: {best_feats}\")\n",
    "            print(f\"mean_pos: {best_feats_mean_pos:.2f}\")\n",
    "            print(f\"top1: {best_successes[1]:.2f}, top2: {best_successes[2]:.2f}, top5: {best_successes[5]:.2f}, top10: {best_successes[10]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3260c9-c0fa-4828-a749-4d34499abacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = 500\n",
    "retries = 200\n",
    "nfeats = trange(1, 11, leave=False, desc=\"nfeats\")\n",
    "nattack = trange(50, 350, 50, leave=False, desc=\"nattack\")\n",
    "num_workers = 30\n",
    "\n",
    "selected_random_euclid = find_features_random(feats_in_tree, nfeats, nattack, num_workers, retries, simulations, euclid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a5868-e92c-4b84-9f19-664627d9848a",
   "metadata": {},
   "source": [
    "## Simulate distinguishing using a Bayes classifier\n",
    "\n",
    "We need to first select some features (divisors) from the set of all divisors that we will query\n",
    "the target with. This set should be the smallest (to not do a lot of queries) yet allow us to distinguish as\n",
    "much as possible.\n",
    "\n",
    "Then, we can build a true Bayes classifier. Since our features are conditionally independent (when conditioned on the class label) in our case naive Bayes == non-naive Bayes. We examine four feature selection algorithms:\n",
    " - Feature selection by pre-selection using tree-building and final selection by random subsets + classification error.\n",
    " - Feature selection via greedy classification error.\n",
    " - Feature selection via mRMR (maximal relevance, minimal redundancy) using mutual information.\n",
    " - Feature selection via JMI (Joint Mutual Information)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81e076-9ccb-445d-ada9-384b73efb2c5",
   "metadata": {},
   "source": [
    "### Feature selection using trees + classification error\n",
    "\n",
    "We can reuse the clustering + tree building approach above and just take the inputs that the greedy tree building choses as the features. However, we can also use more conventional feature selection approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24b323-3604-4e34-a880-9dfd611fb245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feats_in_tree = Counter()\n",
    "for node in PreOrderIter(tree.root):\n",
    "    if node.is_leaf:\n",
    "        continue\n",
    "    feats_in_tree[node.dmap_input] += 1\n",
    "feats_in_tree = set(feats_in_tree.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1052222-ad32-4e25-97ca-851cc42bf546",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = 500\n",
    "retries = 200\n",
    "nfeats = trange(1, 11, leave=False, desc=\"nfeats\")\n",
    "nattack = trange(50, 350, 50, leave=False, desc=\"nattack\")\n",
    "num_workers = 30\n",
    "\n",
    "selected_random_bayes = find_features_random(feats_in_tree, nfeats, nattack, num_workers, retries, simulations, bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df03e89-d517-4243-bbfb-a5f52de24bb1",
   "metadata": {},
   "source": [
    "### Feature selection via greedy classification\n",
    "We can also use the classifier itself for feature selection. We iterate over all the divisors to pick the first feature with the best classifier results in simulation. Then we iteratively add features to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c778a4-0855-4248-91a9-750fdd76ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features_greedy(nfeats, nattack, num_workers, simulations, scorer, start_features=None):\n",
    "    available_feats = selected_divisors\n",
    "    feats = []\n",
    "    if start_features is not None:\n",
    "        if nfeats <= len(start_features):\n",
    "            raise ValueError(\"Features already picked.\")\n",
    "        feats.extend(start_features)\n",
    "        for feat in start_features:\n",
    "            available_feats.remove(feat)\n",
    "\n",
    "    with TaskExecutor(max_workers=num_workers) as pool:\n",
    "        while len(feats) < nfeats:\n",
    "            for feat in available_feats:\n",
    "                pool.submit_task(feat,\n",
    "                                 many_simulations,\n",
    "                                 nattack, distributions_mults, feats + [feat], scorer, simulations)\n",
    "            best_feat = None\n",
    "            best_feat_mean_pos = None\n",
    "            best_successes = Noned\n",
    "            for feat, future in tqdm(pool.as_completed(), total=len(available_feats), desc=f\"Picking feature {len(feats)}\", smoothing=0):\n",
    "                mean_pos, successes = future.result()\n",
    "                if best_feat is None or best_feat_mean_pos > mean_pos:\n",
    "                    best_feat = feat\n",
    "                    best_feat_mean_pos = mean_pos\n",
    "                    best_successes = successes\n",
    "            print(f\"Picked {best_feat} with mean pos: {mean_pos:.2f}\")\n",
    "            print(f\"top1: {best_successes[1]:.2f}, top2: {best_successes[2]:.2f}, top5: {best_successes[5]:.2f}, top10: {best_successes[10]:.2f}\")\n",
    "            feats.append(best_feat)\n",
    "            available_feats.remove(best_feat)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c2313-83b0-43f8-80d6-14c39be0d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeats = 5\n",
    "nattack = 100\n",
    "num_workers = 30\n",
    "simulations = 500\n",
    "scorer = bayes\n",
    "\n",
    "selected_greedy = find_features_greedy(nfeats, nattack, num_workers, simulations, scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c030a1c-c13d-401a-bcdb-212c064681e4",
   "metadata": {},
   "source": [
    "### Feature selection via mRMR using mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd769175-c188-411c-af36-2973e7a0ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(class_priors, p_ci_list, n):\n",
    "    \"\"\"\n",
    "    Compute mutual information I(X; Y) for a binomial feature with given class parameters.\n",
    "    \n",
    "    Args:\n",
    "        class_priors (np.array): P(Y=c), shape (num_classes,)\n",
    "        p_ci_list (np.array): Binomial parameters [p_{c,i}] for each class c, shape (num_classes,)\n",
    "        n (int): Number of trials in binomial distribution\n",
    "    \n",
    "    Returns:\n",
    "        float: Mutual information I(X; Y)\n",
    "    \"\"\"\n",
    "    num_classes = len(class_priors)\n",
    "    \n",
    "    # Precompute all PMFs across x and classes\n",
    "    x_values = np.arange(0, n + 1)[:, None]  # (n+1, 1)\n",
    "    pmfs = binom.pmf(x_values, n, p_ci_list[None, :])  # Shape: (n+1, num_classes)\n",
    "    \n",
    "    # Compute joint probabilities P(Y=c) * P(X=x | Y=c)\n",
    "    # Multiply class_priors (shape C) with each row of pmfs (each x has shape (C,))\n",
    "    # class_priors[None, :] becomes (1, C), so broadcasting works.\n",
    "    joint_probs = pmfs * class_priors[None, :]\n",
    "    \n",
    "    # Compute P(X=x) for all x\n",
    "    px = np.sum(joint_probs, axis=1)\n",
    "\n",
    "    # Compute H(Y|X):\n",
    "    h_ygx = 0.0\n",
    "\n",
    "    for x_idx in range(n + 1):\n",
    "        current_px = px[x_idx]\n",
    "        \n",
    "        if current_px < 1e-9:  # Skip negligible probabilities\n",
    "            continue\n",
    "        \n",
    "        cond_probs = joint_probs[x_idx] / current_px  # P(Y=c | X=x)\n",
    "        \n",
    "        # Compute entropy H(Y|X=x) using scipy's entropy function\n",
    "        h_x = entropy(cond_probs, base=2)\n",
    "        \n",
    "        h_ygx += current_px * h_x\n",
    "    \n",
    "    # Prior entropy H(Y)\n",
    "    h_y = entropy(class_priors, base=2)\n",
    "\n",
    "    return h_y - h_ygx\n",
    "\n",
    "\n",
    "def mutual_information_between_features(class_priors, p_ci_i, p_ci_j, n):\n",
    "    \"\"\"\n",
    "    Compute mutual information between two features X_i and X_j.\n",
    "    \n",
    "    Parameters:\n",
    "        class_priors (array): Prior probabilities of each class. Shape: (num_classes,)\n",
    "        p_ci_i (array): Binomial parameters for feature i across classes. Shape: (num_classes,)\n",
    "        p_ci_j (array): Binomial parameters for feature j across classes. Shape: (num_classes,)\n",
    "        n (int): Number of trials for the binomial distribution.\n",
    "    \n",
    "    Returns:\n",
    "        float: Mutual information I(X_i; X_j)\n",
    "    \"\"\"\n",
    "    num_classes = len(class_priors)\n",
    "    x_vals = np.arange(0, n + 1)  # Possible values of features\n",
    "    \n",
    "    ### Compute marginal distributions P(Xi=x), P(Xj=y) ###\n",
    "    # PMF for feature i across all classes\n",
    "    pmf_i_per_class = binom.pmf(x_vals[:, None], n, p_ci_i[None, :])\n",
    "    px_i = np.sum(pmf_i_per_class * class_priors[None, :], axis=1)\n",
    "    entropy_xi = entropy(px_i, base=2) if not np.allclose(px_i, 0.0) else 0.0\n",
    "    \n",
    "    # PMF for feature j across all classes\n",
    "    pmf_j_per_class = binom.pmf(x_vals[:, None], n, p_ci_j[None, :])\n",
    "    px_j = np.sum(pmf_j_per_class * class_priors[None, :], axis=1)\n",
    "    entropy_xj = entropy(px_j, base=2) if not np.allclose(px_j, 0.0) else 0.0\n",
    "    \n",
    "    ### Compute joint distribution P(Xi=x, Xj=y) ###\n",
    "    joint_xy = np.zeros((n + 1, n + 1))\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        pmf_i_c = binom.pmf(x_vals, n, p_ci_i[c])\n",
    "        pmf_j_c = binom.pmf(x_vals, n, p_ci_j[c])\n",
    "        \n",
    "        # Outer product gives joint PMF for class c\n",
    "        outer = np.outer(pmf_i_c, pmf_j_c)\n",
    "        joint_xy += class_priors[c] * outer\n",
    "    \n",
    "    # Compute entropy of the joint distribution\n",
    "    epsilon = 1e-10  # To avoid log(0) issues\n",
    "    non_zero = (joint_xy > epsilon)\n",
    "    entropy_joint = -np.sum(joint_xy[non_zero] * np.log2(joint_xy[non_zero]))\n",
    "    \n",
    "    ### Mutual Information ###\n",
    "    mi = entropy_xi + entropy_xj - entropy_joint\n",
    "    \n",
    "    return mi\n",
    "\n",
    "\n",
    "def conditional_mutual_info(class_priors, XJ_params, XK_params, n):\n",
    "    \"\"\"\n",
    "    Compute I(XK; Y | XJ) using vectorization with broadcasting.\n",
    "    \n",
    "    Args:\n",
    "        XJ_params (array): p_{c,J} for all classes c.\n",
    "        XK_params (array): p_{c,K} for all classes c.\n",
    "        class_priors (array): P(Y=c) for all classes c.\n",
    "        n (int): Number of trials in the binomial distribution.\n",
    "\n",
    "    Returns:\n",
    "        float: Conditional mutual information I(XK; Y | XJ).\n",
    "    \"\"\"\n",
    "    K = len(class_priors)\n",
    "    x_values = np.arange(n + 1)\n",
    "\n",
    "    # Precompute PMFs for each class\n",
    "    P_XJ_giv_Y = binom.pmf(x_values[:, None], n, XJ_params)  \n",
    "    P_XK_giv_Y = binom.pmf(x_values[:, None], n, XK_params)  \n",
    "\n",
    "    P_XJ_T = P_XJ_giv_Y.T  # Shape: (K, n+1)\n",
    "    P_XK_T = P_XK_giv_Y.T\n",
    "\n",
    "    ######################################################################\n",
    "    ### Compute H(Y | XJ) ###############################################\n",
    "    ######################################################################\n",
    "\n",
    "    # Calculate P(XJ=xj) for all xj\n",
    "    P_XJ_total = np.dot(class_priors, P_XJ_T)\n",
    "\n",
    "    # Numerators of posterior probabilities P(Y=c | XJ=xj)\n",
    "    numerators_YgXJ = class_priors[:, None] * P_XJ_T  \n",
    "\n",
    "    valid_mask = P_XJ_total > 1e-9\n",
    "    posterior_YgXJ = np.zeros_like(numerators_YgXJ, dtype=float)\n",
    "    posterior_YgXJ[:, valid_mask] = (\n",
    "        numerators_YgXJ[:, valid_mask] / \n",
    "        P_XJ_total[valid_mask]\n",
    "    )\n",
    "\n",
    "    log_p = np.log2(posterior_YgXJ + 1e-9)  \n",
    "    entropy_terms_HYgXJ = -np.sum(\n",
    "        posterior_YgXJ * log_p, \n",
    "        axis=0,\n",
    "        where=(posterior_YgXJ > 1e-9)\n",
    "    )\n",
    "    \n",
    "    H_Y_given_XJ = np.dot(P_XJ_total, entropy_terms_HYgXJ)\n",
    "\n",
    "    ######################################################################\n",
    "    ### Compute H(Y | XJ, XK) ###########################################\n",
    "    ######################################################################\n",
    "\n",
    "    # Broadcast to compute joint PMF P(XJ=xj, XK=xk | Y=c)\n",
    "    P_XJ_giv_Y_T = P_XJ_T[..., None]  # Shape: (K, n+1, 1)\n",
    "    P_XK_giv_Y_T = P_XK_T[:, None, :]  # Shape: (K, 1, n+1)\n",
    "\n",
    "    joint_pmf_conditional = (\n",
    "        P_XJ_giv_Y_T * \n",
    "        P_XK_giv_Y_T\n",
    "    )  # Shape: (K, n+1, n+1)\n",
    "\n",
    "    numerators = class_priors[:, None, None] * joint_pmf_conditional  \n",
    "\n",
    "    denominators = np.sum(numerators, axis=0)  # Shape: (n+1, n+1)\n",
    "\n",
    "    valid_mask_3d = (denominators > 1e-9)[None, ...]  # Expand for class dimension\n",
    "\n",
    "    # Compute posterior probabilities using broadcasting and where\n",
    "    posterior_YgXJXK = numerators / denominators[None, ...]\n",
    "    posterior_YgXJXK = np.where(valid_mask_3d, posterior_YgXJXK, 0.0)\n",
    "\n",
    "    log_p_joint = np.log2(posterior_YgXJXK + 1e-9)  \n",
    "    entropy_terms_HYgXJXK = -np.sum(\n",
    "        posterior_YgXJXK * log_p_joint,\n",
    "        axis=0,  # Sum over classes (axis 0 is K)\n",
    "        where=(posterior_YgXJXK > 1e-9),\n",
    "    )\n",
    "\n",
    "    H_Y_given_XJXK = np.sum(denominators * entropy_terms_HYgXJXK)\n",
    "\n",
    "    ######################################################################\n",
    "    ### Compute CMI #####################################################\n",
    "    ######################################################################\n",
    "\n",
    "    cmi = H_Y_given_XJ - H_Y_given_XJXK\n",
    "\n",
    "    return max(cmi, 0.0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03acb79a-040e-4bc7-a235-fd80dd72addb",
   "metadata": {},
   "source": [
    "#### Relevance and redundancy\n",
    "First, lets pre-compute the relevance and redundancy metrics for mRMR (also used in JMI). We assume a uniform class prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febfb392-017f-442f-8aaa-cb48bcdb9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = np.full(nmults, 1/nmults, dtype=np.float64)\n",
    "probs = np.zeros((nallfeats, nmults), dtype=np.float64)\n",
    "for i, divisor in enumerate(allfeats):\n",
    "    for j, (mult, probmap) in enumerate(distributions_mults.items()):\n",
    "        probs[i, j] = probmap[divisor]\n",
    "\n",
    "nattack = 100\n",
    "mis = []\n",
    "relevance = np.zeros(nallfeats, dtype=np.float64)\n",
    "for i, divisor in enumerate(allfeats):\n",
    "    mi = mutual_information(priors, probs[i, ], nattack)\n",
    "    relevance[i] = mi\n",
    "    mis.append((mi, divisor))\n",
    "mis.sort(key=lambda item: item[0], reverse=True)\n",
    "\n",
    "print(\"Top 10 feats\")\n",
    "for mi, divisor in mis[:10]:\n",
    "    print(f\"{divisor} {mi:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361d1a3-87d1-4d35-9a8a-1a9a4a6eb638",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 30\n",
    "\n",
    "redundancy = np.zeros((nallfeats, nallfeats), dtype=np.float64)\n",
    "with TaskExecutor(max_workers=num_workers) as pool:\n",
    "    for i in trange(nallfeats):\n",
    "        for j in range(nallfeats):\n",
    "            if i < j:\n",
    "                continue\n",
    "            pool.submit_task((i, j),\n",
    "                             mutual_information_between_features,\n",
    "                             priors, probs[i, ], probs[j, ], nattack)\n",
    "        for (i, j), future in pool.as_completed():\n",
    "            mi = future.result()\n",
    "            redundancy[i][j] = mi\n",
    "            redundancy[j][i] = mi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0b366-188f-4c65-b92c-e9b9587a8083",
   "metadata": {},
   "source": [
    "Store the relevance and redundancy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64cf0a1-a83f-4837-8113-5de536fb0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"relevance.pickle\", \"wb\") as f:\n",
    "    pickle.dump(relevance, f)\n",
    "with open(\"redundancy.pickle\", \"wb\") as f:\n",
    "    pickle.dump(redundancy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3223edc-f3b2-4137-bc1f-75e311ff075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrmr_selection(relevance, redundancy, nfeats):\n",
    "    \"\"\"\n",
    "    Select top features using mRMR.\n",
    "    \n",
    "    Returns:\n",
    "        indices of selected features.\n",
    "    \"\"\"\n",
    "    selected_indices = []\n",
    "    remaining_indices = list(range(nallfeats))\n",
    "    \n",
    "    # Initialize by selecting the most relevant feature\n",
    "    first_feature_idx = np.argmax(relevance)\n",
    "    selected_indices.append(first_feature_idx)\n",
    "    remaining_indices.remove(first_feature_idx)\n",
    "    \n",
    "    while len(selected_indices) < nfeats:\n",
    "        candidates_scores = []\n",
    "        \n",
    "        for candidate in remaining_indices:\n",
    "            # Compute mRMR score: relevance - average redundancy with selected features\n",
    "            current_relevance = relevance[candidate]\n",
    "            \n",
    "            avg_red = 0.0\n",
    "            if len(selected_indices) > 0:\n",
    "                sum_red = np.sum(redundancy[candidate][selected_indices])\n",
    "                avg_red = sum_red / len(selected_indices)\n",
    "            \n",
    "            score = current_relevance - avg_red\n",
    "            candidates_scores.append(score)\n",
    "        \n",
    "        # Select the candidate with highest score\n",
    "        best_candidate_idx = remaining_indices[np.argmax(candidates_scores)]\n",
    "        selected_indices.append(best_candidate_idx)\n",
    "        remaining_indices.remove(best_candidate_idx)\n",
    "    \n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5604d599-ef63-49fc-a7c8-65fa90c15620",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_mrmr = [allfeats[i] for i in mrmr_selection(relevance, redundancy, nfeats=5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b75cd-3c62-4b87-a7df-f0c5f7748386",
   "metadata": {},
   "source": [
    "### Feature selection via JMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3f827-baef-49c2-af60-0be74ff0efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jmi_selection(features_params_list, class_priors, n_trials, relevance, nfeats):\n",
    "    \"\"\"\n",
    "    Select top features using JMI.\n",
    "    \n",
    "    Returns:\n",
    "        indices of selected features.\n",
    "    \"\"\"\n",
    "    selected_indices = []\n",
    "    remaining_indices = list(range(nallfeats))\n",
    "    \n",
    "    # Initialize by selecting the most relevant feature\n",
    "    first_feature_idx = np.argmax(relevance)\n",
    "    selected_indices.append(first_feature_idx)\n",
    "    remaining_indices.remove(first_feature_idx)\n",
    "    \n",
    "    while len(selected_indices) < nfeats:\n",
    "        candidates_scores = []\n",
    "        \n",
    "        for candidate in tqdm(remaining_indices):\n",
    "            # Compute mRMR score: relevance - average redundancy with selected features\n",
    "            current_relevance = relevance[candidate]\n",
    "            \n",
    "            sum_cmi = 0.0\n",
    "            for selected in selected_indices:\n",
    "                XJ_params = features_params_list[selected]\n",
    "                XK_params = features_params_list[candidate]\n",
    "                \n",
    "                cmi_val = conditional_mutual_info(\n",
    "                    class_priors=class_priors,\n",
    "                    XJ_params=XJ_params,\n",
    "                    XK_params=XK_params,\n",
    "                    n=n_trials\n",
    "                )\n",
    "                sum_cmi += cmi_val\n",
    "            avg_cmi = sum_cmi / len(selected_indices)\n",
    "            score = current_relevance + avg_cmi\n",
    "            candidates_scores.append(score)\n",
    "        \n",
    "        # Select the candidate with highest score\n",
    "        best_candidate_idx = remaining_indices[np.argmax(candidates_scores)]\n",
    "        selected_indices.append(best_candidate_idx)\n",
    "        remaining_indices.remove(best_candidate_idx)\n",
    "    \n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6739192e-879a-4862-b862-6a8fc3939b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_jmi = [allfeats[i] for i in jmi_selection(probs, priors, nattack, relevance, nfeats=5)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
