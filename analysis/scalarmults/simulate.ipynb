{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805d746e-610b-4d40-80d2-a8080a993f96",
   "metadata": {},
   "source": [
    "# Simulating EPA-RE using points of low-order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4386513-cc14-434b-a748-2863f8657452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import gc\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html.\",\n",
    "    category=UserWarning\n",
    ")\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from random import randint, randbytes\n",
    "from typing import Type, Any\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from pyecsca.ec.params import DomainParameters, get_params\n",
    "from pyecsca.ec.mult import *\n",
    "from pyecsca.ec.mod import mod\n",
    "from pyecsca.sca.re.rpa import multiple_graph\n",
    "from pyecsca.sca.re.epa import graph_to_check_inputs, evaluate_checks\n",
    "from pyecsca.misc.utils import TaskExecutor\n",
    "\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b156d2a-7345-47f8-a76e-71a7d2be9d22",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463a7bd-34d8-458b-8ceb-dddf99de21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silence():\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html.\",\n",
    "        category=UserWarning\n",
    "    )\n",
    "silence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c11fc-86cf-4eb1-bf4e-b2e44b2d7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmults = len(all_mults)\n",
    "nmults_ctr = len(all_mults_with_ctr)\n",
    "nerror_models = len(all_error_models)\n",
    "ncfgs = nmults_ctr * nerror_models\n",
    "\n",
    "print(f\"Scalar multipliers considered:  {nmults}\")\n",
    "print(f\"Scalar multipliers (with a single countermeasure) considered:  {nmults_ctr}\")\n",
    "print(f\"Error models considered:  {nerror_models}\")\n",
    "print(f\"Total configurations considered:  {ncfgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e9543-8447-4362-b9e2-c896d71f69a9",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c7f10-618f-4612-b594-81d1607b0d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"secg\"\n",
    "curve = \"secp256r1\"\n",
    "kind = \"precomp+necessary\"\n",
    "use_init = True\n",
    "use_multiply = True\n",
    "params = get_params(category, curve, \"projective\")\n",
    "num_workers = 20\n",
    "bits = params.order.bit_length()\n",
    "samples = 100\n",
    "selected_mults = all_mults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc266d-35eb-4f6d-bdba-e9f6f66827f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_multiples(mult: MultIdent,\n",
    "                       params: DomainParameters,\n",
    "                       bits: int,\n",
    "                       samples: int = 100,\n",
    "                       use_init: bool = True,\n",
    "                       use_multiply: bool = True,\n",
    "                       seed: bytes | None = None) -> MultResults:\n",
    "    results = []\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # If no countermeasure is used, we have fully random scalars.\n",
    "    # Otherwise, fix one per chunk.\n",
    "    if mult.countermeasure is None:\n",
    "        scalars = [random.randint(1, 2**bits) for _ in range(samples)]\n",
    "    else:\n",
    "        one = random.randint(1, 2**bits)\n",
    "        scalars = [one for _ in range(samples)]\n",
    "\n",
    "    for scalar in scalars:\n",
    "        results.append(multiple_graph(scalar, params, mult.klass, mult.partial, use_init, use_multiply))\n",
    "    return MultResults(results, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64799c16-8113-4eff-81de-6a3e547eb5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiples(mult: MultIdent, res: MultResults, divisors: set[int]):\n",
    "    errors = {divisor: 0 for divisor in divisors}\n",
    "    samples = len(res)\n",
    "    divisors_hash = hashlib.blake2b(str(sorted(divisors)).encode(), digest_size=8).digest()\n",
    "    for ctx, out in res:\n",
    "        check_inputs = graph_to_check_inputs(ctx, out,\n",
    "                                             check_condition=mult.error_model.check_condition,\n",
    "                                             precomp_to_affine=mult.error_model.precomp_to_affine)\n",
    "        for q in divisors:\n",
    "            error = evaluate_checks(check_funcs={\"add\": mult.error_model.check_add(q), \"affine\": mult.error_model.check_affine(q)},\n",
    "                                    check_inputs=check_inputs)\n",
    "            errors[q] += error\n",
    "    # Make probmaps smaller. Do not store zero probabilities.\n",
    "    probs = {}\n",
    "    for q, error in errors.items():\n",
    "        if error != 0:\n",
    "            probs[q] = error / samples\n",
    "    return ProbMap(probs, divisors_hash, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac630a08-4120-41cf-b3bb-1827ef469542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiples_direct(mult: MultIdent, fname: str, offset: int, divisors: set[int]):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        f.seek(offset)\n",
    "        _, res = pickle.load(f)\n",
    "    errors = {divisor: 0 for divisor in divisors}\n",
    "    samples = len(res)\n",
    "    divisors_hash = hashlib.blake2b(str(sorted(divisors)).encode(), digest_size=8).digest()\n",
    "    for ctx, out in res:\n",
    "        check_inputs = graph_to_check_inputs(ctx, out,\n",
    "                                             check_condition=mult.error_model.check_condition,\n",
    "                                             precomp_to_affine=mult.error_model.precomp_to_affine)\n",
    "        for q in divisors:\n",
    "            error = evaluate_checks(check_funcs={\"add\": mult.error_model.check_add(q), \"affine\": mult.error_model.check_affine(q)},\n",
    "                                    check_inputs=check_inputs)\n",
    "            errors[q] += error\n",
    "    # Make probmaps smaller. Do not store zero probabilities.\n",
    "    probs = {}\n",
    "    for q, error in errors.items():\n",
    "        if error != 0:\n",
    "            probs[q] = error / samples\n",
    "    return ProbMap(probs, divisors_hash, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf712e-5b97-4390-8dd4-e1db1dfe36a2",
   "metadata": {},
   "source": [
    "## Run\n",
    "Run this cell as many times as you want. It will write chunks into files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84359084-4116-436c-92cd-d43fdfeca842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunk_id = randbytes(4).hex()\n",
    "with TaskExecutor(max_workers=num_workers, initializer=silence) as pool:\n",
    "    for mult in all_mults_with_ctr:\n",
    "        pool.submit_task(mult,\n",
    "                         simulate_multiples,\n",
    "                         mult, params, bits, samples, seed=chunk_id, use_init=use_init, use_multiply=use_multiply)\n",
    "    with open(f\"multiples_{bits}_{'init' if use_init else 'noinit'}_{'mult' if use_multiply else 'nomult'}_chunk{chunk_id}.pickle\",\"wb\") as h:\n",
    "        for mult, future in tqdm(pool.as_completed(), desc=\"Computing multiple graphs.\", total=len(pool.tasks)):\n",
    "            print(f\"Got {mult}.\")\n",
    "            if error := future.exception():\n",
    "                print(\"Error!\", error)\n",
    "                continue\n",
    "            res = future.result()\n",
    "            pickle.dump((mult, res), h)\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44120f28-ae4a-42e3-befb-ebc487d51f9e",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab8333-b8f1-4890-b38a-7bb34f5ffb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with TaskExecutor(max_workers=num_workers, initializer=silence) as pool:\n",
    "    for in_fname in tqdm(glob.glob(f\"multiples_{bits}_{'init' if use_init else 'noinit'}_{'mult' if use_multiply else 'nomult'}_chunk*.pickle\"), desc=\"Processing chunks\", smoothing=0):\n",
    "        \n",
    "        match = re.match(\"multiples_(?P<bits>[0-9]+)_(?P<init>(?:no)?init)_(?P<mult>(?:no)?mult)_chunk(?P<id>[0-9a-f]+).pickle\", in_fname)\n",
    "        bits = match.group(\"bits\")\n",
    "        use_init = match.group(\"init\")\n",
    "        use_multiply = match.group(\"mult\")\n",
    "        chunk_id = match.group(\"id\")\n",
    "        out_fname = f\"probs_{bits}_{use_init}_{use_multiply}_chunk{chunk_id}.pickle\"\n",
    "\n",
    "\n",
    "        in_file = Path(in_fname)\n",
    "        out_file = Path(out_fname)\n",
    "\n",
    "        cfgs_todo = set()\n",
    "        for mult in all_mults_with_ctr:\n",
    "            for error_model in all_error_models:\n",
    "                cfgs_todo.add(mult.with_error_model(error_model))\n",
    "\n",
    "        if out_file.exists():\n",
    "            print(f\"Processing chunk {chunk_id}, some(or all) probmaps found.\")\n",
    "            with out_file.open(\"r+b\") as f:\n",
    "                while True:\n",
    "                    try:\n",
    "                        full, _ = pickle.load(f)\n",
    "                        cfgs_todo.remove(full)\n",
    "                        last_end = f.tell()\n",
    "                    except EOFError:\n",
    "                        break\n",
    "                    except pickle.UnpicklingError:\n",
    "                        f.truncate(last_end)\n",
    "            if not cfgs_todo:\n",
    "                print(f\"Chunk complete. Continuing...\")\n",
    "            else:\n",
    "                print(f\"Chunk missing {len(cfgs_todo)} probmaps, computing...\")\n",
    "        else:\n",
    "            print(f\"Processing chunk {chunk_id}, no probmaps found.\")\n",
    "        \n",
    "        with in_file.open(\"rb\") as f, out_file.open(\"ab\") as h:\n",
    "            loading_bar = tqdm(total=nmults_ctr, desc=f\"Loading chunk {chunk_id}.\", smoothing=0)\n",
    "            processing_bar = tqdm(total=len(cfgs_todo), desc=f\"Processing {chunk_id}.\", smoothing=0)\n",
    "            while True:\n",
    "                try:\n",
    "                    start = f.tell()\n",
    "                    mult, vals = pickle.load(f)\n",
    "                    loading_bar.update(1)\n",
    "                    for error_model in all_error_models:\n",
    "                        full = mult.with_error_model(error_model)\n",
    "                        if full in cfgs_todo:\n",
    "                            # Pass the file name and offset to speed up computation start.\n",
    "                            pool.submit_task(full,\n",
    "                                             evaluate_multiples_direct,\n",
    "                                             full, in_fname, start, divisor_map[\"all\"])\n",
    "                    gc.collect()\n",
    "                    if len(pool.tasks) > 1000:\n",
    "                        for full, future in pool.as_completed():\n",
    "                            processing_bar.update(1)\n",
    "                            if error := future.exception():\n",
    "                                print(\"Error!\", full, error)\n",
    "                                continue\n",
    "                            res = future.result()\n",
    "                            pickle.dump((full, res), h)\n",
    "                except EOFError:\n",
    "                    break\n",
    "                except pickle.UnpicklingError:\n",
    "                    print(\"Bad unpickling, the multiples file is likely truncated.\")\n",
    "                    break\n",
    "            for full, future in pool.as_completed():\n",
    "                processing_bar.update(1)\n",
    "                if error := future.exception():\n",
    "                    print(\"Error!\", full, error)\n",
    "                    continue\n",
    "                res = future.result()\n",
    "                pickle.dump((full, res), h)\n",
    "        print(\"Chunk completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228922dc-67bf-481a-9f08-4084695e2059",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdaad25-80d2-4574-9cfb-9d93e55e90d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyinstrument import Profiler as PyProfiler\n",
    "mult = next(iter(multiples_mults))\n",
    "res = multiples_mults[mult]\n",
    "\n",
    "\n",
    "for checks in powerset(checks_add):\n",
    "    for precomp_to_affine in (True, False):\n",
    "        for check_condition in (\"all\", \"necessary\"):\n",
    "            error_model = ErrorModel(checks, check_condition=check_condition, precomp_to_affine=precomp_to_affine)\n",
    "            full = mult.with_error_model(error_model)\n",
    "            print(full)\n",
    "            #with PyProfiler() as prof:\n",
    "            probmap = evaluate_multiples(full, res, divisor_map[\"all\"])\n",
    "            #print(prof.output_text(unicode=True, color=True))\n",
    "            #print(probmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d291832-b0c7-4c3a-9989-22079e4e0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiples_mults = {}\n",
    "for fname in glob.glob(f\"multiples_{bits}_{'init' if use_init else 'noinit'}_{'mult' if use_multiply else 'nomult'}_chunk*.pickle\"):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                mult, vals = pickle.load(f)\n",
    "                if mult not in multiples_mults:\n",
    "                    multiples_mults[mult] = vals\n",
    "                else:\n",
    "                    multiples_mults[mult].merge(vals)\n",
    "            except EOFError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba5215-fef8-4c8a-8d7d-1af49edffa7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
